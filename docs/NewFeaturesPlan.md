Okay, this is an excellent focus! Implementing these Tier 1 features will significantly enhance the value of Report-AI. Here's a detailed implementation plan, keeping SOLID, KISS, and DRY principles in mind.

## Overall Guiding Principles for Tier 1 Implementation:

*   **SOLID:**
    *   **SRP (Single Responsibility Principle):** New services/modules will handle specific tasks (e.g., missing info detection, section revision). API endpoints will be granular.
    *   **OCP (Open/Closed Principle):** Design so that adding new clarification types or AI editing actions is an extension, not a modification of core logic.
    *   **DIP (Dependency Inversion Principle):** Continue to rely on abstractions for LLM interactions and other services.
*   **KISS (Keep It Simple, Stupid):**
    *   Start with the most straightforward way to achieve the functionality.
    *   For "Missing Info," initially focus on explicitly empty/placeholder values from the LLM.
    *   For "Preview & Edit," begin with a simple display and direct text editing before complex AI interactions.
*   **DRY (Don't Repeat Yourself):**
    *   Reuse existing components (e.g., LLM call wrappers, `doc_builder.inject`).
    *   Centralize prompt templates and data structures where possible.

## Data Structures & State Management (Preliminary)

1.  **`ReportContext` (Python Pydantic Model / TypedDict, Frontend JS Object):**
    *   This will be the central data structure holding all information for the report. It will include all fields currently in `final_ctx` (client details, dates, textual sections like `dinamica_eventi`, `accertamenti`, etc.).
    *   This structure will be generated by the backend, sent to the frontend for preview/editing, and sent back to the backend for final DOCX generation or AI revisions.
2.  **`InitialRequestContext` (Internal Backend):**
    *   May be needed to store the state of a generation request if it's paused for user clarification (e.g., uploaded file paths, original notes, RAG results, request ID). This could be managed via a short-lived cache or by having the frontend resubmit necessary context. *KISS approach: have frontend resubmit what's needed.*

---

## Feature 1: Interactive Missing Information & Clarification

**Goal:** If the initial LLM pass fails to extract key information, prompt the user for it before proceeding with full report generation.

### Phase 1.1: Backend - Detection & Signalling

1.  **Modify LLM Prompt for Base Fields (`build_prompt.jinja2`):**
    *   Instruct the LLM to explicitly use a unique placeholder (e.g., `"[MISSING: <field_description>]"`) or return `null` for any of the defined JSON keys if the information cannot be found in the provided `corpus`, `notes`, or `images`. This is more reliable than just checking for empty strings.
    *   Example: `{"polizza": "[MISSING: Numero Polizza]"}` or `{"polizza": null}`.

2.  **Backend Logic in `app/api/routes.py` (or a new service):**
    *   After the initial `call_llm` for base fields (the one currently populating `base_ctx`):
        *   Parse the LLM's JSON response.
        *   **New Service (`ClarificationService` or similar - SRP):**
            *   `identify_missing_fields(llm_response: dict) -> List[Dict[str, str]]`: This function iterates through the `llm_response`. If it finds the `[MISSING: ...]` placeholder or `null` for predefined critical fields, it compiles a list.
            *   Each item in the list could be `{"key": "polizza", "prompt_question": "Per favore, inserisci il numero di polizza:"}`. The `prompt_question` can be predefined or even partially generated by the LLM based on `field_description`.
    *   **Modified `/api/generate` Endpoint Logic:**
        *   If `identify_missing_fields` returns a non-empty list:
            *   Store the current state of the request (uploaded file contents/paths, notes, RAG results, initial `llm_response` before clarification). *KISS: Instead of server-side state, the frontend will resubmit this context later.*
            *   Return a specific JSON response to the frontend, e.g.:
                ```json
                {
                  "status": "clarification_needed",
                  "missing_fields": [
                    {"key": "polizza", "label": "Numero Polizza", "current_value": null, "question": "Qual è il numero di polizza?"},
                    {"key": "data_danno", "label": "Data Danno", "current_value": null, "question": "Qual è la data del danno?"}
                  ],
                  // To avoid re-uploading, pass back references or essential data:
                  "request_artifacts": { // Data frontend needs to send back
                      "original_corpus": "...", // Or a reference if too large
                      "image_tokens": ["..."],
                      "notes": "...",
                      "use_rag": true,
                      "similar_cases_retrieved": [], // if RAG was used
                      "initial_llm_base_fields": {} // The full initial output
                  }
                }
                ```
        *   If no missing fields, proceed as currently implemented (run pipeline, inject, return DOCX).

### Phase 1.2: Frontend - Clarification UI

1.  **Modify `frontend/script.js`:**
    *   In the `form.addEventListener('submit', ...)` handler:
        *   If the response from `/api/generate` has `status === "clarification_needed"`:
            *   Hide the main spinner.
            *   Display a new "Clarification UI" section/modal.
            *   Dynamically generate input fields based on the `missing_fields` array. Use `label` for display and `key` for the input name.
            *   Store `request_artifacts` received from the backend (e.g., in a global JS variable or hidden form fields).
            *   Add a new "Submit Clarifications & Generate Report" button to this UI.
2.  **New Clarification Form Submission:**
    *   When the "Submit Clarifications" button is clicked:
        *   Collect the user's answers from the dynamically generated form.
        *   Construct a new payload.
        *   **New Endpoint Call:** Send a `POST` request to a new endpoint, e.g., `/api/generate-with-clarifications`.
            *   Payload:
                ```json
                {
                  "clarifications": {"polizza": "USER_INPUT_123", "data_danno": "USER_INPUT_DATE"},
                  "request_artifacts": { /* The artifacts received earlier */ }
                }
                ```

### Phase 1.3: Backend - Processing Clarifications & Final Generation

1.  **New Endpoint (`POST /api/generate-with-clarifications`):**
    *   **SRP:** This endpoint's responsibility is to take the initial context + user clarifications and complete the report.
    *   **Input:** The JSON payload described above.
    *   **Logic:**
        1.  Retrieve/Reconstruct the initial context using `request_artifacts`.
        2.  Merge `clarifications` provided by the user into the `initial_llm_base_fields` from `request_artifacts`. This updated dictionary becomes the new `base_ctx`.
        3.  **DRY:** From this point, proceed with the existing report generation pipeline:
            *   `pipeline = PipelineService()`
            *   `section_map = await pipeline.run(...)` using the updated `base_ctx` and other elements from `request_artifacts` (corpus, imgs, notes, similar_cases).
            *   `final_ctx = {**base_ctx, **section_map}`
            *   `docx_bytes = inject(template_path, json.dumps(final_ctx))`
            *   Return the `StreamingResponse` with the DOCX file.
    *   Error handling for this new endpoint is also crucial.

---

## Feature 2: Preview & Guided In-App Editing

**Goal:** Allow users to see the generated report content within the app, make direct text edits, and (later) request AI-driven revisions before downloading.

### Phase 2.A: Backend - Return Structured Data for Preview

1.  **Modify `/api/generate` (and `/api/generate-with-clarifications`):**
    *   Instead of immediately generating and returning DOCX bytes (unless no clarification is needed and no preview is desired, which can be a flag), these endpoints will now have a different primary return path.
    *   After all LLM processing (base fields extraction via `call_llm` and main sections via `PipelineService.run`) is complete, they will assemble the `final_ctx` (the complete `ReportContext` data).
    *   Return this `final_ctx` as a JSON response to the frontend.
        ```json
        {
          "status": "preview_ready",
          "report_data": { /* The full ReportContext object */ }
        }
        ```
    *   The current DOCX generation logic (`inject` and `StreamingResponse`) will be moved to a new dedicated endpoint.

2.  **New Endpoint (`POST /api/download-report`):**
    *   **SRP:** Its sole responsibility is to take a complete `ReportContext` JSON and produce a DOCX file.
    *   **Input:**
        ```json
        {
          "report_data": { /* The (potentially edited) ReportContext object from frontend */ }
        }
        ```
    *   **Logic (DRY):**
        1.  `final_ctx = request.json()["report_data"]`
        2.  `docx_bytes = inject(template_path, json.dumps(final_ctx))`
        3.  Return `StreamingResponse` with DOCX.

### Phase 2.B: Frontend - Basic Preview UI

1.  **Modify `frontend/script.js`:**
    *   When `/api/generate` (or `/api/generate-with-clarifications`) returns `status === "preview_ready"`:
        *   Hide the main spinner/clarification UI.
        *   Display a new "Report Preview" UI section/page.
        *   Take the `report_data` object.
        *   Dynamically render the content:
            *   Simple fields (client, date, etc.) can be displayed as `label: value`.
            *   Main textual sections (`dinamica_eventi`, `accertamenti`, etc.) can be displayed within `<p>` tags or `<div>`s.
            *   **KISS:** Start with a basic, readable HTML representation.
        *   Add a "Download Report" button to this preview UI.
2.  **"Download Report" Button Logic:**
    *   When clicked, it takes the current (initially unedited) `report_data` from its client-side state.
    *   Sends this `report_data` to the `POST /api/download-report` endpoint.
    *   Handles the DOCX file download as currently implemented.

### Phase 2.C: Frontend - Direct Text Editing

1.  **Enhance Preview UI (`frontend/script.js` & `index.html` structure):**
    *   For the main textual sections (`dinamica_eventi`, `accertamenti`, `quantificazione`, `commento`) in the preview:
        *   Instead of static `<div>`s or `<p>`s, render them inside `<textarea>` elements, pre-filled with the content from `report_data`.
    *   **Client-Side State:** Maintain a JavaScript object that mirrors the `report_data`. When a `textarea`'s content changes (e.g., on `input` or `blur` event), update the corresponding property in this client-side `report_data` object.
2.  **"Download Report" Button Logic (Update):**
    *   Now, when clicked, it sends the *potentially modified* client-side `report_data` object to `POST /api/download-report`.

### Phase 2.D: Backend - AI-Powered Section Revisions

1.  **New Endpoint (`POST /api/revise-section`):**
    *   **SRP:** Handles AI-driven revision for a single report section.
    *   **Input:**
        ```json
        {
          "section_key": "dinamica_eventi", // Key of the section to revise
          "current_text": "Text content of the section...",
          "revision_instruction": "Make this section more concise.", // Or "Expand on point X."
          "original_context": { // Minimal necessary context for the LLM to understand the revision
            "corpus_summary": "...", // A summary or key excerpts from original corpus
            "notes": "...",
            "template_excerpt": "..." // To maintain style
          }
        }
        ```
    *   **Logic:**
        1.  Retrieve input data.
        2.  **New Prompt Template (`revise_section_prompt.jinja2` - DRY):**
            *   This template will take `current_text`, `revision_instruction`, and `original_context` to instruct the LLM on how to revise the section.
            *   Example: *"You are an expert insurance report editor. Given the following section text, the user's revision instruction, and original case context, please provide the revised section text. Original section: '{{current_text}}'. User instruction: '{{revision_instruction}}'. Context: '{{original_context.corpus_summary}}'. Respond ONLY with the revised text for the section, no extra explanations."*
        3.  `prompt = build_revision_prompt(...)`
        4.  `raw_llm_response = await call_llm(prompt)` (This LLM call might need a different system prompt, e.g., "Respond only with the revised text.")
        5.  `revised_text = raw_llm_response.strip()` (Assuming LLM returns only the text, not JSON here for simplicity, adjust if JSON parsing is more robust).
        6.  Return JSON: `{"revised_text": revised_text}`.

### Phase 2.E: Frontend - AI Revision UI & Interaction

1.  **Enhance Preview UI (`frontend/script.js`):**
    *   Next to each editable `textarea` (for main sections):
        *   Add predefined action buttons (e.g., "Make Concise," "Expand," "Formalize").
        *   Optionally, add a small text input for a custom revision instruction.
2.  **AI Revision Button Logic:**
    *   When an AI revision button/action is triggered for a section:
        *   Get the `section_key`, current text from the `textarea`, and the `revision_instruction` (either from the button's predefined value or the custom input).
        *   Gather `original_context` (this might need to be passed down from the initial `/api/generate` call or intelligently summarized on the client/backend). *KISS: Initially, maybe just pass the original notes and template excerpt.*
        *   Show a loading indicator for that specific section.
        *   Call `POST /api/revise-section` with the payload.
        *   On success, update the `textarea` with the `revised_text` from the response.
        *   Update the client-side `report_data` object with this new text.
        *   Hide the section-specific loading indicator.

---

This detailed plan breaks down the features into manageable backend and frontend tasks, adhering to good software design principles. Each phase builds upon the previous one, allowing for iterative development and testing. Remember to version control frequently and write tests for new backend logic!
